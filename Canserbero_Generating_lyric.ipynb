{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "import nltk\n",
    "import numpy as np \n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpu = gpus[0]\n",
    "tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/raudez77/The-art-of-state/tree/main\n",
    "import Crawler_Canserbero\n",
    "import Pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_idx(c):\n",
    "    if c in chars:\n",
    "        return char2idx[c]\n",
    "    return char2idx[UNK]\n",
    "\n",
    "def AICanserbero (Vocab_size,EMD,Batch_size,l1_units,l2_units):\n",
    "    model= tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(Vocab_size,EMD, mask_zero=True,\n",
    "                                batch_input_shape = [Batch_size,None]))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.GRU(l1_units, return_sequences = True,\n",
    "                                  stateful = True,recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.GRU(l2_units, return_sequences = True,\n",
    "                                  stateful = True,recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(Vocab_size))\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_text (model_ , star_string, temperature =0.7, num_generate = 50):\n",
    "    # Converting string to numbers\n",
    "    input_eval = [char2idx[s] for s in star_string]\n",
    "    input_eval = tf.expand_dims(input_eval,0) #Onehot Econder\n",
    "\n",
    "\n",
    "    # Text Generated\n",
    "    text_generated = []\n",
    "    \n",
    "    # Batch\n",
    "    for i in range(num_generate):\n",
    "        predictions = model_(input_eval)\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        \n",
    "        # Using a categorical distribution \n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return (star_string+ ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.letras.com/canserbero/\"\n",
    "Canser = Crawler_Canserbero(url)\n",
    "song = Canser._song_by_song()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'yourpath'\n",
    "Preprocess = Pre_process(path)\n",
    "all_songs = Preprocess.Clean()\n",
    "print(\"This is sample including in the canserbero_song.txt\")\n",
    "all_songs.split(\",\")[6:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing\n",
    "\n",
    "1.   Create list of unique character in spanish we have accents and special word such as \"ñ\"\n",
    "2.   Define token such as EOS \"End of Sentence\", PAD \"Padding Sentences\"\n",
    "3. Use encoding \"utf-8\" for special character \n",
    "4. Define Max length for the sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "chars = sorted(set(\"\"\"áéèíóúüabcdefghijklmnñopqrstuvwxyz0123456789 -,;.[¡¿!?]:“'’’’/\\|_@#$%ˆ&*˜‘+-=()[]{}\"'ÁÉÍÓÚABCDEFGHIJKLMNÑOPQRSTUVWXYZ\"\"\"))\n",
    "chars = list(chars)\n",
    "EOS = '<EOS>' #End of the sentences \n",
    "UNK = \"<UNK>\" #unknowd Character\n",
    "PAD = \"<PAD>\" #Padding\n",
    "chars.append(UNK)\n",
    "chars.append(EOS)  #end of sentence\n",
    "chars.insert(0, PAD)  # now padding should get index of 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Create a dictionary with unique characters and give them a unique number\n",
    "2.   Load the data set defined by the Max_length \"The number of character per sentences \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(chars)}\n",
    "idx2char = np.array(chars)\n",
    "\n",
    "data = []\n",
    "MAX_LEN = 75 #Value that repeat the most is  45  \n",
    "with open (path , \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = csv.reader(file, delimiter = '\\t')\n",
    "    for line in lines:\n",
    "        sln = line[0]\n",
    "        covrtd = [char_idx(c) for c in sln[:-1]]\n",
    "        if len(covrtd) >= MAX_LEN:\n",
    "            covrtd = covrtd[0:MAX_LEN-1]\n",
    "            covrtd.append(char2idx[EOS]) #EOS End of the line\n",
    "        else:\n",
    "            covrtd.append(char2idx[EOS])\n",
    "            #adding padding\n",
    "            remain = MAX_LEN - len(covrtd)\n",
    "            if remain > 0 :\n",
    "                for i in range(remain):\n",
    "                    covrtd.append(char2idx[PAD])\n",
    "        data.append(covrtd)\n",
    "print(\"**** Data file loaded ****\")\n",
    "print(f\"This an one sentences,'{''.join([idx2char[i] for i in data[0][:20]])}'\")\n",
    "print(\"This is the same sentences but already tokenized\",[i for i in  data[0][:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to numpy \n",
    "np_data = np.array(data)\n",
    "\n",
    "# Training and Testing \n",
    "np_data_in, np_data_out  = np_data[:,:-1] , np_data[:, 1:]\n",
    "\n",
    "# Creating Tensor\n",
    "X = tf.data.Dataset.from_tensor_slices((np_data_in,np_data_out))\n",
    "\n",
    "\n",
    "# Settings\n",
    "Vocab_size = len(chars)\n",
    "EMD = 64\n",
    "RRN_unit = 1024\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Creating data set \n",
    "X_train = X.shuffle(1000, reshuffle_each_iteration=True).\\\n",
    "batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "Vocab_size = len(chars)\n",
    "EMD = 64\n",
    "l1_units = 1024\n",
    "l2_units = 560\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = AICanserbero (Vocab_size,EMD,BATCH_SIZE,l1_units,l2_units)\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer = 'adam', loss = loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was train with a batch_size of `[128,64]` , So now if I want to generate some text I should pass 128 rows but to avoid that:\n",
    "1. I will extact the weights\n",
    "2. Initiate a new model with same parameter but the input will be 1,None which means one line whith N number of character\n",
    "3. Set the Weights using the old model weight\n",
    "4. Predict new song in a foor lop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model_Weigths = model.get_weights()\n",
    "AI_CANSERBERO = AICanserbero(Vocab_size,EMD,1,l1_units,l2_units)\n",
    "AI_CANSERBERO.set_weights(old_model_Weigths)\n",
    "AI_CANSERBERO.build(tf.TensorShape([1,None]))\n",
    "\n",
    "\n",
    "new_lyrics = generate_text(AI_CANSERBERO , star_string = 'con un pueblo culto',temperature =0.8,\n",
    "                   num_generate =10)\n",
    "new_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
